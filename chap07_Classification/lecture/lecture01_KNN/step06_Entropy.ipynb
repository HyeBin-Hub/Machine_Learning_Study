{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Gini 불순도(impurity), Entropy\n",
    " - Tree model에서 중요변수 선정 기준 \n",
    " - 확률 변수 간의 불확실성을 나타내는 수치, 작을 수록 불확실성이 낮다.\n",
    " - 입력 변수(x) -> 중요 변수 선정 \n",
    " - 정보이득 = base 지수 - Gini 불순도 or entropy(정보이득이 클 수록 중요변수)\n",
    " - Gini 불순도 = sum(p * (1-p))\n",
    " - Entropy = -sum(p * log(p)) \n",
    "   -> 확률(p)의 log값이 음수로 나오기 때문에 음수(-)를 붙여서 양수로 변경   \n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "# 1. 불확실성이 큰 경우(x1 :앞면이 나올수 잇는 혹률 ,x2:뒷면이 나올수 잇는 확률)\n",
    "x1=0.5; x2=0.5 # -> 불확식성이 높은 확률 예제\n",
    "p=x1+x2 # 전체확률\n",
    "gini=(x1/p)*(1-x1/p)+(x2/p)*(1-x2/p) # (sum을 +로 해줌)\n",
    "print(gini) # 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy = 1.0\n"
     ]
    }
   ],
   "source": [
    "entropy=-x1*np.log2(x1)-x2*np.log2(x2)\n",
    "# entropy=-sum([x1*np.log2(x1),x2*np.log2(x2)]) 위랑 똑같은 결과, 안쪽 리스트로 만들어서 각각 원소로 만들어줌[1,2]\n",
    "print('entropy =', entropy) # 1.0\n",
    "# -> 지니랑 엔트로피의 수치가 작으면 불확실성이 낮다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost= 1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "cost function : 정답과 예측치 간의 오차 함수로 사용 \n",
    " x1 -> y_true, x2 -> y_pred\n",
    " cost = -sum(x, y) # entropy\n",
    "'''\n",
    "y_true = x1 * np.log2(x1) # 정답 \n",
    "y_pred = x2 * np.log2(x2) # 예측치 \n",
    "\n",
    "cost = -sum([y_true, y_pred]) # -sum(x1 * np.log2(x1), x2 * np.log2(x2))\n",
    "print('cost=', cost) # cost1= 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gini= 0.18\n"
     ]
    }
   ],
   "source": [
    "# 2. 불확실성이 작은 경우 (x1 :앞면이 나올수 잇는 확률 ,x2:뒷면이 나올수 잇는 확률)\n",
    "x1=0.9; x2=0.1 # -> 불확식성이 낮은 확률 예제\n",
    "p=x1+x2 # 전체확률\n",
    "\n",
    "gini=(x1/p)*(1-x1/p)+(x2/p)*(1-x2/p) # (sum을 +로 해줌)\n",
    "print('\\ngini=', gini) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4689955935892812\n"
     ]
    }
   ],
   "source": [
    "entropy=-x1*np.log2(x1)-x2*np.log2(x2)\n",
    "# entropy=-sum([x1*np.log2(x1),x2*np.log2(x2)]) 위랑 똑같은 결과, 안쪽 리스트로 만들어서 각각 원소로 만들어줌[1,2]\n",
    "print(entropy) # 0.4689955935892812\n",
    "# -> 지니랑 엔트로피의 수치가 작으면 불확실성이 낮다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4689955935892812\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "cost function : 정답과 예측치 간의 오차 함수 , 엔트로피를 기반으로함(오차가 얼마나 심한지 알려주는 함수)\n",
    "x1 -> y_true\n",
    "x2 -> y_pred\n",
    "\"\"\"\n",
    "y_true = x1*np.log2(x1) # 정답이라고 가졍\n",
    "y_pred = x2*np.log2(x2) # 가정\n",
    "# entropy는 정답과 예측치 간의 오차 함수로 사용 (둘의 오차가 작으면 잘 분류한것)\n",
    "cost = -sum([y_true,y_pred]) # loss funtion(손실함수)\n",
    "print(cost)  # 0.4689955935892812 위의 엔트로피와 수피 같음 (0에 수렴할수록 오차가 작다,정확하게 예측한것)\n",
    "# n개의 관측치 : 관측치의 평균  \n",
    "# cost = -np.mean(x1 * np.log2(x1) + x2 * np.log2(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################\n",
    "##dataset적용\n",
    "################\n",
    "# 1.data set 생성 함수\n",
    "from math import log\n",
    "\n",
    "# 1. data set 생성 함수\n",
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    columns = ['dark_clouds','gust'] #X1,X2,label\n",
    "    return dataSet, columns\n",
    "# 함수 호출\n",
    "dataSet, columns = createDataSet()\n",
    "print(dataSet,type(dataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' '1' 'yes']\n",
      " ['1' '1' 'yes']\n",
      " ['1' '0' 'no']\n",
      " ['0' '1' 'no']\n",
      " ['0' '1' 'no']]\n"
     ]
    }
   ],
   "source": [
    "dataSet = np.array(dataSet) # 선형대수 연산을하기 위해numpy로 형변환하기\n",
    "print(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "['dark_clouds', 'gust']\n"
     ]
    }
   ],
   "source": [
    "print(dataSet.shape) #(5, 3) -> x1,x2,label\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y변수 선택\n",
    "x = dataSet[:,:2] # x1,x2  행전체 사용 열은 두개\n",
    "y=dataSet[:, -1] # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# label -> dummy 생성\n",
    "label = [1 if i ==\"yes\" else 0 for i in y] #  i가 yes면1 아니면 0 (list + for)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n"
     ]
    }
   ],
   "source": [
    "# tree model 시각화\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score #  model 평가\n",
    "\n",
    "# model 오브젝트 생성\n",
    "obj=DecisionTreeClassifier() # default\n",
    "#obj=DecisionTreeClassifier(criterion='entropy') \n",
    "model=obj.fit(x,label)\n",
    "print(model)\n",
    "# max_depth=None: 트리가 깊을수록 오뷴류 감소(과적합 증가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(x)\n",
    "\n",
    "acc=accuracy_score(label,y_pred)\n",
    "print(acc) #1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree grapg\n",
    "export_graphviz(model,out_file = \"dataset_graph.dot\",feature_names=columns,max_depth=5,class_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
