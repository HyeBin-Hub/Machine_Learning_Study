{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[생략] 동물보호관리시스템 홈페이지 \n",
    "http://www.animal.go.kr\n",
    "\n",
    "<url pramater>\n",
    "s_date : 시작년도\n",
    "e_date : 종료년도 \n",
    "s_upr_cd : 시도 \n",
    "pagecnt : 페이지 번호 \n",
    "'''\n",
    "\n",
    "import urllib.request as req  # url 가져오기 \n",
    "from bs4 import BeautifulSoup # tag 데이터 가져오기 \n",
    "\n",
    "'''\n",
    "년도 : 2010-01-01 ~ 오늘\n",
    "시도 : 서울시 , 부산시, 대구시 \n",
    "페이지 : 1~10\n",
    "'''\n",
    "\n",
    "base_url = \"http://www.animal.go.kr/portal_rnl/abandonment/public_list.jsp?s_date={s_date}&e_date={e_date}&s_upr_cd={city}&s_org_cd=0000000&s_up_kind_cd=&s_kind_cd=&s_name=&s_shelter_cd=&s_wrk_cd=&s_state=&s_state_hidden=&pagecnt=\"\n",
    "crawling_data = [] # data 저장 list\n",
    "city_data = {'서울' : '6110000', '부산' : '6260000', '대구':'6270000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링 동물 수 = 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 클로러 함수(시작년도, 종료년도, 도시, 페이지번호) \n",
    "def crawler_func(s_date, e_date, city, pg=10): \n",
    "    # url 구성 = base_url + parameter\n",
    "    url = base_url.format(s_date=s_date, e_date=e_date, city=city_data[city])\n",
    "    \n",
    "    # page 번호(page 수 만큼 반복)\n",
    "    for p in range(1, int(pg)+1) : # pg=10 = 1 ~ 10\n",
    "        # 1) 최종 url 구성\n",
    "        url2 = url + str(p) \n",
    "        #print(url2)\n",
    "        \n",
    "        # 2) url 요청 -> html source\n",
    "        res = req.urlopen(url2)\n",
    "        data = res.read()\n",
    "        \n",
    "        # 3) html 파싱\n",
    "        html = data.decode('UTF8') # charset='euc-kr'\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # 4) tag 기반 crawling : 태그[속성='값'] > 태그[속성='값'] \n",
    "        '''\n",
    "        <div class=\"thumb_inner02\">\n",
    "        <dl class=\"thumbnail_table01\">\n",
    "        '''\n",
    "        gonggos = soup.select('div[class=thumb_inner02] > dl[class=thumbnail_table01]')\n",
    "        \n",
    "        for gonggo in gonggos : # page 당 동물 10 정보\n",
    "            # <dd> 태그 추출 \n",
    "            dds = gonggo.find_all('dd') # 1동물 당 7개 칼럼 \n",
    "            # print(dds)\n",
    "            \n",
    "            ani_one = [] # 1동물 당 7개 칼럼 저장\n",
    "            for dd in dds :\n",
    "                re = dd.string.strip()\n",
    "                #print(re) # <dd> 태그 내용 불용어 제거(엔터,공백,특수문자)\n",
    "                ani_one.append(re) # [dd1, ~ dd7]\n",
    "            \n",
    "            # 중첩list : [[dd1, ~ dd7], ~ [dd1, ~ dd7]]\n",
    "            crawling_data.append(ani_one)    \n",
    "\n",
    "# 클로러 함수 호출 \n",
    "crawler_func('2018-01-01', '2018-01-30', '부산')\n",
    "print('크롤링 동물 수 =', len(crawling_data)) # 10\n",
    "print(crawling_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.frame\n",
    "import pandas as pd\n",
    "\n",
    "# vector\n",
    "c1=[]; c2=[]; c3=[]; c4=[]; c5=[]; c6=[]; c7=[]  \n",
    "for one in crawling_data : # 100번 반복 \n",
    "    c1.append(one[0]); c2.append(one[1]);c3.append(one[2])  \n",
    "    c4.append(one[3]); c5.append(one[4]);c6.append(one[5])\n",
    "    c7.append(one[6]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   no       0 non-null      float64\n",
      " 1   date     0 non-null      float64\n",
      " 2   kind     0 non-null      float64\n",
      " 3   gender   0 non-null      float64\n",
      " 4   place    0 non-null      float64\n",
      " 5   feature  0 non-null      float64\n",
      " 6   state    0 non-null      float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 124.0 bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# data.frame 생성\n",
    "cols = ['no','date','kind','gender','place','feature','state']\n",
    "crawling_df = pd.DataFrame({'no' : c1, 'date' : c2, 'kind' : c3, 'gender':c4,\n",
    "             'place' : c5, 'feature' : c6, 'state' : c7},\n",
    "             columns=cols)\n",
    "\n",
    "print(crawling_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [no, date, kind, gender, place, feature, state]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# file save\n",
    "\n",
    "crawling_df.to_csv(\"C:/Users/hyebin/Desktop/Python_ML/data/crawling_df.csv\",\n",
    "                    index=None, encoding='utf-8')\n",
    "\n",
    "\n",
    "crawling = pd.read_csv(\"C:/Users/hyebin/Desktop/Python_ML/data/crawling_df.csv\")\n",
    "print(crawling.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
